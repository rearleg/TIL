# 2024년 12월 17일 ML 수업 TILM
## 지도학습 방법론

### 1. ML방법론의 분류

##### 1) Supervision과 지도학습

- Supervision
입력 데이터에 대해 결과가 정답으로 나와야하는지 직접 지정해 주는 것
즉, 사람이 설정한 라벨(y)

##### 2) 비지도학습이란?

- 비지도학습
레이블(y)를 활용하지 않고 입력 데이터(x)로만 모델을 학습하는 방법론

### 2. 회기모델의 정의와 개념 소개

##### 1) 회귀모델의 개념 정의

1. 회귀모델 : 연속현 변수의 예측
  - 회귀 모델이란?
    - 출력값이 연속형 변수인 모델을 사용하는 방법론
    - e.g. 부모와 자녀의 키, 오늘과 내일의 기온

  - 입력 변수는 여러개 이거나 범주형이라도 상관 없다.
    - 거주 지역, 나이, 성별 -> 연간 소득금액
    - 수익 관련 지표 -> 1주일 후 주식 가격
  - 출력이 여러개
    - 사람 얼굴 이미지 -> 눈, 코, 입의 위치

##### 2) 선형회귀 모델의 원리와 해법

1. 회귀 모델의 세부 구분
  - 입력 변수가 여러개인가?
    - 1개 : 단순 회귀분석
    - 다수 : 다중 회귀분석
  - 출력 변수가 여러개인가?
    - 1개 : (단변량) 회귀분석
    - 다수 : 다변량 회귀분석
  - 사용하는 모델이 선형 모델인가?
    - 선형 회귀분석
    - 비선형 회귀분석

2. 단순 선형 회귀모델
  - Line Fitting 관점에서 선형회귀모델
    - 직선의 기울기(a)와 y절편(b)을 조절해 데이터(점들)와의 오차가 가장 작아지도록 만들기
  - MSE 손실함수
    - 오차 목표값과 출력값 간 차이를 제곱으로 정의되는 Mean Squared Error 손실함수 사용
  - 최적화 문제로서의 선형 회귀
    - convex 최적화 문제

3. 최적화의 수치적 해법과 해석적 해법
  - 해석적 해법
    - 미리 알려진 해법만 사용 가능
    - 아주 빠르고 간편하게 정확한 값을 구함
  - 수치적 해법
    - 결과가 근사값으로 나옴
    - 시간이 필요함
    - 많은 경우에만 사용 가능

##### 3) 단순 선형 회귀모델과 상관관계 분석

1. 상관분석과 상관계수
  - 상관관계 : 함께 변화하는 경향성
  - 인과관계 : 변화가 원인이 되어 결과로서 다른 변수를 변화시킬 때
  - 상관관계 분석 : 두 변수 사이의 상관관계 분석
  - 피어슨 상관계수
    - 분석의 결과로 두 변수 사이의 선형상관관계가 있는지를 -1 ~ 1 범위로 나타낸 것

2. 상관관계 분석의 가정
  - 선형성 Linearity : 두 변수의 상관관계가 선형적인가?
    - 오른쪽 데이터는 매우 강한 비선형적 상관관계지만, 단순선형 회귀모델 분석은 0에 가까운 계
  - 등분산성: 입력 범수의 범위에 대해 잔차의 분산이 동일해야한다는 개념
  - 정규분포성 : 각 변수는 모두 정규분포를 따름
    - 일반적으로 분석 전에 표준화 방식의 전처리를 진행해준다.
  - 독립성 : 각 샘플들은 모두 독립적으로 추출 되어야 한다.
  - 상관행렬
    - 모든 변수 쌍의 조합에 대한 상관계수 행렬
    - 대칭행렬로 나오지만 의미 없는 중복값과 대각선 원소를 제거
    - 변수간 상호 연관성

3. 상관계수 행렬의 시각화
  - 공선성과 다중공선성
    - 상관계수가 1.0, -1.0으로 나타나는 경우를 공선성이 있다고 한다.
    - 다중공선성은 여러 변수들의 선형결합으로 나타남
    - 공선성, 다중공선성이 나타나지 않은 때까지 변수를 제거하는 전처리 진행

4. 상관분석과 선형회귀의 관계
  - 상관계수는 직선의 기울기 X

### 3. 분류 모델의 정의와 개념 소개

##### 1) 선형모델을 활용한 분류문제 모델

1. 이진분류문제와 로지스틱 회귀
  - 이진분류 Binary Classification
    - 출력값이 참 혹은 거짓으로만 나오는 가장 간단한 분류 모델

2. 로지스틱 회귀모델의 학습
  - 선형회귀모델 결과 그대로 확률로 해석할 경우 문제가 생김
    - 출력 결과가 0~1을 벗어날 수 없음
  - 로지스틱 함수 : 0~1로 인풋을 변환해 확률로 해석될 수 있는 결과를 내보내주는 함수
  - 시그모이드 함수 : S자형 곡선그래프가 나오는 함수, 로지스틱 함수를 포함해 다양한 예시들이 있음
  - 딥러닝에서는 : 시그모이드 함수는 보통 로지스틱 함수이다.
  - 크로스엔트로피 : 로지스틱 함수를 통과한 모델의 출력값 확률에 대해 적용하도록 만들어진 손실함수
    - Binary Cross Entropy
    - 크로스엔트로피 사용 이유
      - BCE를 MSE보다 학습이 잘된다.
      - 예측 결과가 완전히 틀린 경우 bce는 매우 큰 패널티를 주지만 mse는 그렇지 않음
      - 0.8 등 정답에 어느정도 근접한 결과를 내놓은 경우 mse는 로스 값이 너무 작아져 학습이 매우 느려짐
    - 로지스틱 함수와 결합했을 때 미분식이 매우 간단해짐

3. 다중분류문제의 모델링
  - 원 핫 인코딩 : 정답 클래스가 i일 경우 i번째 원소만 1이고 나머지 값이 0인 벡터로 라벨을 표현
  - 소프트맥스 : 다중 분 모델에서 결과 Logit 벡터를 각각 클래스에 대한 확률을 나태내는 벡터로 변환해주도록 설계된 함수
  - 

##### 2))  다양한 방식의 분류문제 모델링

1. k-최근접 이웃(kNN) 알고리즘 k-nearest neighbor
: 특정 인풋 시 학습 데이터셋에서 가장 근접한 k개의 라벨을 기준으로 출력값을 결정
    
    
    - k=3인 경우(실선) : 빨강 2개 , 파랑 1개 → 빨강으로 분류
    - k=5인 경우(점선) : 빨강 2개, 파랑 3개 → 파랑으로 분류됨
    
2. 결정트리와 랜던 포레스트
    - 결정트리(의사결정나무, decision tree)
    : 독립변수 X내의 대소 관계나 특정 임계값과의 비교, 판단하여 계층적으로 적용 최종 결과 출력
    → 스무고개
    - 불순도(impuriy)
    : 한 범주 안에 데이터가 서로 얼마나 섞여 있는지를 측정하는 지표,
    → 결정트리 학습과정에서 손실함수와 같은 역할을 함
    - 결정포레스트
    : 결정트리 여러개를 종합해 최종 출력을 결정
    → 여러 모델을 합쳐서 더 나은 최종 성능을 내는 것을 앙상블이라 한다.
    
3. SVM을 활용한 이진분류
    - 겨정경계
    : 이진분류 모델에서 판단의 기준이 되는 초평면
    → 결정경계를 기준으로 공간의 양쪽이 서로 다른 클래스로 분류된다.
    - 선형불리가능한 데이터셋
    : 하나의 결정경계를 통해 이진분류 데이터셋의 두 클래스가 완전히 나뉘어질 수 있을 때
    이 데이터셋은 선형불리가 가능하다고 한다.
    - 서포트 벡터 머신
    : 선형분리 가능한 데이터셋에서, 데이터의 두 클래스를 가장 잘 분리하는 결정경계를 찾아내기 위한 방법
    - 커널 트릭 (Kernel Trick)
    : 커널함수를 통해 고차원으로 변환 → 선형분리가 가능해지도록 만들고 svm을 적용
